{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otrZdXl58mPH"
   },
   "source": [
    "# Continual Learning for pedestrian detection (CrowdHuman to CityScape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rfI2u3bMYWOL",
    "outputId": "0fb5a812-babd-4d3c-bcc3-ecef75645553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cuda_enable = True\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda:0\")\n",
    "  print(\"GPU\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print(\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pRHpDpHtYWOQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nls0kzaOYWOY"
   },
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, annotation, transforms=None):\n",
    "    self.root = root\n",
    "    self.transforms = transforms\n",
    "    self.coco = COCO(annotation)\n",
    "    self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # image ID\n",
    "    img_id = self.ids[idx]\n",
    "    # image file_name\n",
    "    img_file = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "    # read_imgae\n",
    "    img = Image.open(os.path.join(self.root, img_file))\n",
    "    # get annotation ID\n",
    "    ann_ids = self.coco.getAnnIds(imgIds = img_id)\n",
    "    # read annotation\n",
    "    anns = self.coco.loadAnns(ann_ids)\n",
    "    # num of people in the picture\n",
    "    num_objs = len(anns)\n",
    "    # build information about bounding box & area\n",
    "    boxes = []\n",
    "    areas = []\n",
    "    for i in range(num_objs):\n",
    "      x_min = anns[i]['bbox'][0]\n",
    "      y_min = anns[i]['bbox'][1]\n",
    "      x_max = x_min + anns[i]['bbox'][2]\n",
    "      y_max = y_min + anns[i]['bbox'][3]\n",
    "      boxes.append([x_min, y_min, x_max, y_max])\n",
    "  \n",
    "    # transfer information to Tensor\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    labels = torch.ones((num_objs,), dtype = torch.int64)\n",
    "    img_id = torch.tensor([img_id])\n",
    "    areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "    # Annotation in dict form\n",
    "    Annotations = {\n",
    "        \"boxes\" : boxes,\n",
    "        \"labels\" : labels,\n",
    "        \"image_id\" : img_id,\n",
    "        \"iscrowd\" : iscrowd\n",
    "    }\n",
    "\n",
    "    # transforms\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms(img)\n",
    "    \n",
    "    return img, Annotations\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=2.39s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def get_transforms(train):\n",
    "  trans = []\n",
    "  if train:\n",
    "    trans.append(transforms.RandomHorizontalFlip(0.5))\n",
    "    # trans.append(transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)))\n",
    "    # trans.append(transforms.ColorJitter(brightness=1, contrast=1, saturation=1))\n",
    "  trans.append(transforms.ToTensor())\n",
    "  return transforms.Compose(trans)\n",
    "\n",
    "train_data_path = r'data\\CrowdHuman\\images'\n",
    "coco_path = r\"C:\\Users\\mahdi\\Desktop\\Pedestrian-Detection-master\\data\\CrowdHuman\\train.json\"\n",
    "train_dataset = ImageDataset(root=train_data_path, annotation=coco_path, transforms=get_transforms(train=True))\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, 1, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "8368d0de414a43fda4371db38049c8c9",
      "26a9ac1381cc444aa4945a5228cd8380",
      "632457719d764816a0de63b8510dc1ff",
      "059178c6cca742f9b1fa3085bf860c7e",
      "9075c2aa0716420faf648e8732df91f2",
      "cad656ce7d7e4bd995f386c4fdbcd632",
      "37f60a525f7f4246b9469f93c4610c32",
      "5dd3d0626c9b4171ba7a1e1a21ac273c"
     ]
    },
    "colab_type": "code",
    "id": "P-lnDZIEYWOg",
    "outputId": "8d715eac-6efa-4b63-9248-95c9c20418bb"
   },
   "outputs": [],
   "source": [
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "# change question to binary classification (human, not human)\n",
    "num_classes = 2\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the weights of the model\n",
    "model.load_state_dict(torch.load('ckpt/FRCNN_crowdhuman.pth'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#test data loader\n",
    "class ImageDataset_test(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, annotation, transforms):\n",
    "    self.root = root\n",
    "    self.coco = COCO(annotation)\n",
    "    self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # image ID\n",
    "    img_id = self.ids[idx]\n",
    "    # image file_name\n",
    "    img_file = self.coco.loadImgs(img_id)[0][\"im_name\"]\n",
    "    # read_imgae\n",
    "    img = Image.open(os.path.join(self.root, img_file))\n",
    "\n",
    "    # transforms\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms(img)\n",
    "    \n",
    "    return img\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.ids)\n",
    "\n",
    "test_data_path = r'data\\citypersons\\images\\val'\n",
    "coco_path_test = r\"data\\citypersons\\annotations\\val_gt.json\"\n",
    "test_dataset = ImageDataset_test(root=test_data_path, annotation=coco_path_test, transforms=get_transforms(train=False))\n",
    "\n",
    "    \n",
    "test_loader_city = torch.utils.data.DataLoader(test_dataset, 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.68s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#test data loader\n",
    "class ImageDataset_test(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, annotation, transforms):\n",
    "    self.root = root\n",
    "    self.coco = COCO(annotation)\n",
    "    self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # image ID\n",
    "    img_id = self.ids[idx]\n",
    "    # image file_name\n",
    "    img_file = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "    # read_imgae\n",
    "    img = Image.open(os.path.join(self.root, img_file))\n",
    "\n",
    "    # transforms\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms(img)\n",
    "    \n",
    "    return img\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.ids)\n",
    "\n",
    "test_data_path = r'data\\CrowdHuman\\images\\val'\n",
    "coco_path_test = r\"data\\CrowdHuman\\val.json\"\n",
    "test_dataset = ImageDataset_test(root=test_data_path, annotation=coco_path_test, transforms=get_transforms(train=False))\n",
    "\n",
    "    \n",
    "test_loader_crowd = torch.utils.data.DataLoader(test_dataset, 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def log_output(model,test_loader,epoch,dataset = \"crowdhuman\"):\n",
    "\n",
    "        \n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    res = []\n",
    "    i = 0\n",
    "    for _ in range(1):\n",
    "        for i, imgs in enumerate(test_loader):\n",
    "            model.eval()\n",
    "            imgs = list(img.to(device) for img in imgs)\n",
    "            output = model(imgs)\n",
    "            scores = output[0]['scores'].detach().cpu().numpy()\n",
    "            num_people = len(scores[scores > 0.25])\n",
    "            boxes = output[0]['boxes'].detach().cpu().numpy()\n",
    "            boxes = boxes[:num_people]\n",
    "            if len(boxes) > 0:\n",
    "                boxes[:, [2, 3]] -= boxes[:, [0, 1]]\n",
    "\n",
    "                for ii,box in enumerate(boxes):\n",
    "                    temp = {}\n",
    "                    temp['image_id'] = i+1\n",
    "                    temp['category_id'] = 1\n",
    "                    temp['bbox'] = box[:4].tolist()\n",
    "                    temp['score'] = np.float(scores[ii])\n",
    "                    res.append(temp)\n",
    "                    \n",
    "\n",
    "                print('\\r%d/%d' % (i + 1, len(test_loader)), end='')\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "\n",
    "    with open('_temp_val\\_temp_val_' + dataset + '_%d.json'%epoch, 'w') as f:\n",
    "        json.dump(res, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import autograd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from barbar import Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticWeightConsolidation:\n",
    "\n",
    "    def __init__(self, model, weight=1000000):\n",
    "        self.model = model\n",
    "        self.model.requires_grad_(True)\n",
    "        self.weight = weight\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        self.optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "\n",
    "    def _update_mean_params(self):\n",
    "        for param_name, param in self.model.named_parameters():\n",
    "            _buff_param_name = param_name.replace('.', '__')\n",
    "            self.model.register_buffer(_buff_param_name+'_estimated_mean', param.data.clone())\n",
    "\n",
    "    def _update_fisher_params(self, current_ds, grad_avg_path = None):\n",
    "        dl = current_ds\n",
    "        model.requires_grad_(True)\n",
    "        model.train()\n",
    "        if grad_avg_path is not None:\n",
    "            self.grad_avg = np.load(grad_avg_path, allow_pickle=True)\n",
    "        else:\n",
    "            i = 0\n",
    "            for imgs, annotations in Bar(dl):\n",
    "                imgs = list(img.to(device) for img in imgs)\n",
    "                annotations = [{k:v.to(device) for k, v in t.items()} for t in annotations]\n",
    "\n",
    "                train_flag = False\n",
    "                if len(annotations[0]['boxes'].shape) == 2 or annotations[0]['boxes'].shape[-1] == 4:\n",
    "                    if len(annotations) > 1:\n",
    "                        if (len(annotations[1]['boxes'].shape) == 2 or annotations[1]['boxes'].shape[-1] == 4):\n",
    "                            train_flag = True\n",
    "                    else:\n",
    "                        train_flag = True\n",
    "\n",
    "                if train_flag:\n",
    "                    loss_dict = self.model(imgs, annotations)\n",
    "                    losses = sum(loss for loss in loss_dict.values())  \n",
    "                    log_likelihood = torch.log(losses)\n",
    "                    grad_log_liklihood = autograd.grad(log_likelihood, self.model.parameters())\n",
    "                    grad_log_liklihood = [g.detach().cpu().numpy() for g in grad_log_liklihood]\n",
    "                    # grad_list.append(grad_log_liklihood)\n",
    "\n",
    "                    if i == 0:\n",
    "                        grad_sum = [np.zeros(g.shape) for g in grad_log_liklihood]\n",
    "                    \n",
    "                    for ii in range(len(grad_log_liklihood)):\n",
    "                        grad_sum[ii] += grad_log_liklihood[ii]\n",
    "\n",
    "                    i += 1\n",
    "\n",
    "\n",
    "            self.grad_avg = [g/i for g in grad_sum]\n",
    "        \n",
    "        _buff_param_names = [param[0].replace('.', '__') for param in self.model.named_parameters()]\n",
    "        for _buff_param_name, param in zip(_buff_param_names, self.grad_avg):\n",
    "            self.model.register_buffer(_buff_param_name+'_estimated_fisher', torch.tensor(param).data.clone() ** 2)\n",
    "\n",
    "    def register_ewc_params(self, dataset, grad_avg_path = None):\n",
    "        self._update_fisher_params(dataset, grad_avg_path)\n",
    "        self._update_mean_params()\n",
    "\n",
    "    def _compute_consolidation_loss(self, weight):\n",
    "        try:\n",
    "            losses = []\n",
    "            for param_name, param in self.model.named_parameters():\n",
    "                _buff_param_name = param_name.replace('.', '__')\n",
    "                estimated_mean = getattr(self.model, '{}_estimated_mean'.format(_buff_param_name)).cuda()\n",
    "                estimated_fisher = getattr(self.model, '{}_estimated_fisher'.format(_buff_param_name)).cuda()\n",
    "                \n",
    "                losses.append((estimated_fisher * (param - estimated_mean) ** 2).sum())\n",
    "            return (weight / 2) * sum(losses)\n",
    "        except AttributeError:\n",
    "            return 0\n",
    "\n",
    "    def forward_backward_update(self, input, target):\n",
    "        output = self.model(input)\n",
    "        loss = self._compute_consolidation_loss(self.weight) + self.crit(output, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def Fine_tune_new_data(self, Next_ds, epoch = 15):\n",
    "        params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                    momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                        step_size=3,\n",
    "                                                        gamma=0.1)\n",
    "\n",
    "        epochs = 15\n",
    "        best_epoch = 0\n",
    "        self.model = self.model.to(device)\n",
    "        min_loss = 10000\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            i = 0\n",
    "            print(\"Epoch: %d/%d\" % (epoch, epochs))\n",
    "            loss_sum = 0\n",
    "            for imgs, annotations in Bar(train_loader):\n",
    "                i += 1\n",
    "                imgs = list(img.to(device) for img in imgs)\n",
    "                annotations = [{k:v.to(device) for k, v in t.items()} for t in annotations]\n",
    "                \n",
    "                train_flag = False\n",
    "                if len(annotations[0]['boxes'].shape) == 2 or annotations[0]['boxes'].shape[-1] == 4:\n",
    "                    if len(annotations) > 1:\n",
    "                        if (len(annotations[1]['boxes'].shape) == 2 or annotations[1]['boxes'].shape[-1] == 4):\n",
    "                            train_flag = True\n",
    "                    else:\n",
    "                        train_flag = True\n",
    "\n",
    "                if train_flag:\n",
    "                    loss_dict = model(imgs, annotations)\n",
    "                    losses = self._compute_consolidation_loss(self.weight) + sum(loss for loss in loss_dict.values())\n",
    "                    optimizer.zero_grad()\n",
    "                    losses.backward()\n",
    "                    optimizer.step()\n",
    "                    loss_sum += losses.item()\n",
    "                \n",
    "            print(\"Iteration: {}; Loss: {}\".format(i, loss_sum/i))\n",
    "            if loss_sum/i < min_loss:\n",
    "                min_loss = loss_sum/i\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), 'ckpt/FRCNN_crowdhuman_EWC_CityPersons.pth')\n",
    "                print(\"Model saved\")\n",
    "            print(\"Best epoch: {}; Best loss: {}\".format(best_epoch, min_loss))\n",
    "            lr_scheduler.step()\n",
    "            log_output(model,test_loader_crowd,epoch,dataset = \"crowdhuman\")\n",
    "            log_output(model,test_loader_city,epoch,dataset = \"citypersons\")\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.model, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.model = torch.load(filename)\n",
    "\n",
    "ewc = ElasticWeightConsolidation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000: [===============================>] - ETA 4.3sssss\n"
     ]
    }
   ],
   "source": [
    "# ewc.register_ewc_params(train_loader,\"Average_gradients\\grad_avg_CrowdHuman.npy\")\n",
    "ewc.register_ewc_params(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune with other dataset with EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, annotation, transforms=None):\n",
    "    self.root = root\n",
    "    self.transforms = transforms\n",
    "    self.coco = COCO(annotation)\n",
    "    self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # image ID\n",
    "    img_id = self.ids[idx]\n",
    "    # image file_name\n",
    "    img_file = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "    # read_imgae\n",
    "    img = Image.open(os.path.join(self.root, img_file))\n",
    "    # get annotation ID\n",
    "    ann_ids = self.coco.getAnnIds(imgIds = img_id)\n",
    "    # read annotation\n",
    "    anns = self.coco.loadAnns(ann_ids)\n",
    "    # num of people in the picture\n",
    "    num_objs = len(anns)\n",
    "    # build information about bounding box & area\n",
    "    boxes = []\n",
    "    areas = []\n",
    "    for i in range(num_objs):\n",
    "      x_min = anns[i]['bbox'][0]\n",
    "      y_min = anns[i]['bbox'][1]\n",
    "      x_max = x_min + anns[i]['bbox'][2]\n",
    "      y_max = y_min + anns[i]['bbox'][3]\n",
    "      boxes.append([x_min, y_min, x_max, y_max])\n",
    "  \n",
    "    # transfer information to Tensor\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    labels = torch.ones((num_objs,), dtype = torch.int64)\n",
    "    img_id = torch.tensor([img_id])\n",
    "    areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "    # Annotation in dict form\n",
    "    Annotations = {\n",
    "        \"boxes\" : boxes,\n",
    "        \"labels\" : labels,\n",
    "        \"image_id\" : img_id,\n",
    "        \"iscrowd\" : iscrowd\n",
    "    }\n",
    "\n",
    "    # transforms\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms(img)\n",
    "    \n",
    "    return img, Annotations\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.ids)\n",
    "    \n",
    "\n",
    "def get_transforms(train):\n",
    "  trans = []\n",
    "  if train:\n",
    "    trans.append(transforms.RandomHorizontalFlip(0.5))\n",
    "    # trans.append(transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)))\n",
    "    # trans.append(transforms.ColorJitter(brightness=1, contrast=1, saturation=1))\n",
    "  trans.append(transforms.ToTensor())\n",
    "  return transforms.Compose(trans)\n",
    "\n",
    "train_data_path = r'data\\citypersons\\images'\n",
    "coco_path = r\"C:\\Users\\mahdi\\Desktop\\Pedestrian-Detection-master\\data\\citypersons\\annotations\\train.json\"\n",
    "train_dataset = ImageDataset(root=train_data_path, annotation=coco_path, transforms=get_transforms(train=True))\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, 2, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15\n",
      "2975/2975: [===============================>] - ETA 0.1sss\n",
      "Iteration: 1488; Loss: 0.8074729018579866\n",
      "Model saved\n",
      "Best epoch: 0; Best loss: 0.8074729018579866\n",
      "500/50070Epoch: 1/15\n",
      "2975/2975: [===============================>] - ETA 0.4sss\n",
      "Iteration: 1488; Loss: 0.7820223564011797\n",
      "Model saved\n",
      "Best epoch: 1; Best loss: 0.7820223564011797\n",
      "499/50070Epoch: 2/15\n",
      "2975/2975: [===============================>] - ETA 0.3sss\n",
      "Iteration: 1488; Loss: 0.7661525166340333\n",
      "Model saved\n",
      "Best epoch: 2; Best loss: 0.7661525166340333\n",
      "500/50070Epoch: 3/15\n",
      "2975/2975: [===============================>] - ETA 0.3sss\n",
      "Iteration: 1488; Loss: 0.7062257246322047\n",
      "Model saved\n",
      "Best epoch: 3; Best loss: 0.7062257246322047\n",
      "500/50070Epoch: 4/15\n",
      "2975/2975: [===============================>] - ETA 0.3sss\n",
      "Iteration: 1488; Loss: 0.6896272712810747\n",
      "Model saved\n",
      "Best epoch: 4; Best loss: 0.6896272712810747\n",
      "499/50070Epoch: 5/15\n",
      "2975/2975: [===============================>] - ETA 0.3sss\n",
      "Iteration: 1488; Loss: 0.688249578416992\n",
      "Model saved\n",
      "Best epoch: 5; Best loss: 0.688249578416992\n",
      "500/50070Epoch: 6/15\n",
      "2975/2975: [===============================>] - ETA 0.3sss\n",
      "Iteration: 1488; Loss: 0.6649968298057721\n",
      "Model saved\n",
      "Best epoch: 6; Best loss: 0.6649968298057721\n",
      "500/50070Epoch: 7/15\n",
      "2975/2975: [===============================>] - ETA 0.3sss\n",
      "Iteration: 1488; Loss: 0.6684118045915423\n",
      "Best epoch: 6; Best loss: 0.6649968298057721\n",
      "500/50070Epoch: 8/15\n",
      "2975/2975: [===============================>] - ETA 0.3sss\n",
      "Iteration: 1488; Loss: 0.671413185886923\n",
      "Best epoch: 6; Best loss: 0.6649968298057721\n",
      "499/50070Epoch: 9/15\n",
      "2975/2975: [===============================>] - ETA 0.3sss\n",
      "Iteration: 1488; Loss: 0.6685840162698843\n",
      "Best epoch: 6; Best loss: 0.6649968298057721\n",
      "499/50070Epoch: 10/15\n",
      "2975/2975: [===============================>] - ETA 0.3sss\n",
      "Iteration: 1488; Loss: 0.6697213173586856\n",
      "Best epoch: 6; Best loss: 0.6649968298057721\n",
      "499/50070Epoch: 11/15\n",
      "2975/2975: [===============================>] - ETA 0.3sss\n",
      "Iteration: 1488; Loss: 0.6761211193763146\n",
      "Best epoch: 6; Best loss: 0.6649968298057721\n",
      "499/50070Epoch: 12/15\n",
      "2975/2975: [===============================>] - ETA 0.1sss\n",
      "Iteration: 1488; Loss: 0.6656000050981107\n",
      "Best epoch: 6; Best loss: 0.6649968298057721\n",
      "499/50070Epoch: 13/15\n",
      "2975/2975: [===============================>] - ETA 0.4sss\n",
      "Iteration: 1488; Loss: 0.6701873292404436\n",
      "Best epoch: 6; Best loss: 0.6649968298057721\n",
      "499/50070Epoch: 14/15\n",
      "2975/2975: [===============================>] - ETA 0.3sss\n",
      "Iteration: 1488; Loss: 0.6657829997763419\n",
      "Best epoch: 6; Best loss: 0.6649968298057721\n",
      "499/50070"
     ]
    }
   ],
   "source": [
    "ewc.Fine_tune_new_data(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the weights of the model\n",
    "model.load_state_dict(torch.load('ckpt\\FRCNN_crowdhuman_EWC_CityPersons.pth'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_demo import validate\n",
    "\n",
    "def val(gt_json_path, pred_json_path, log=None):\n",
    "    MRs = validate(gt_json_path, pred_json_path)\n",
    "    print('Summarize: [Reasonable: %.2f%%], [Bare: %.2f%%], [Partial: %.2f%%], [Heavy: %.2f%%]'\n",
    "          % (MRs[0]*100, MRs[1]*100, MRs[2]*100, MRs[3]*100))\n",
    "    if log is not None:\n",
    "        log.write(\"%.7f %.7f %.7f %.7f\\n\" % tuple(MRs))\n",
    "    return MRs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Summarize: [Reasonable: 61.67%], [Bare: 47.85%], [Partial: 64.99%], [Heavy: 87.35%]\n",
      "Summarize: [Reasonable: 36.72%], [Bare: 27.95%], [Partial: 41.69%], [Heavy: 80.12%]\n",
      "1\n",
      "Summarize: [Reasonable: 63.53%], [Bare: 50.28%], [Partial: 62.65%], [Heavy: 81.72%]\n",
      "Summarize: [Reasonable: 32.52%], [Bare: 22.98%], [Partial: 37.24%], [Heavy: 75.33%]\n",
      "2\n",
      "Summarize: [Reasonable: 69.79%], [Bare: 59.33%], [Partial: 69.34%], [Heavy: 86.26%]\n",
      "Summarize: [Reasonable: 47.21%], [Bare: 38.12%], [Partial: 52.35%], [Heavy: 79.52%]\n",
      "3\n",
      "Summarize: [Reasonable: 59.88%], [Bare: 46.45%], [Partial: 58.66%], [Heavy: 81.56%]\n",
      "Summarize: [Reasonable: 24.58%], [Bare: 17.52%], [Partial: 26.99%], [Heavy: 66.35%]\n",
      "4\n",
      "Summarize: [Reasonable: 58.41%], [Bare: 44.82%], [Partial: 57.24%], [Heavy: 80.45%]\n",
      "Summarize: [Reasonable: 24.89%], [Bare: 17.74%], [Partial: 27.39%], [Heavy: 67.22%]\n",
      "5\n",
      "Summarize: [Reasonable: 58.51%], [Bare: 45.09%], [Partial: 57.63%], [Heavy: 81.00%]\n",
      "Summarize: [Reasonable: 24.65%], [Bare: 17.22%], [Partial: 27.05%], [Heavy: 64.69%]\n",
      "6\n",
      "Summarize: [Reasonable: 58.56%], [Bare: 45.17%], [Partial: 57.91%], [Heavy: 81.30%]\n",
      "Summarize: [Reasonable: 24.68%], [Bare: 16.81%], [Partial: 27.12%], [Heavy: 66.24%]\n",
      "7\n",
      "Summarize: [Reasonable: 58.72%], [Bare: 45.41%], [Partial: 58.23%], [Heavy: 81.61%]\n",
      "Summarize: [Reasonable: 24.14%], [Bare: 17.06%], [Partial: 26.59%], [Heavy: 66.27%]\n",
      "8\n",
      "Summarize: [Reasonable: 58.65%], [Bare: 45.31%], [Partial: 57.78%], [Heavy: 81.35%]\n",
      "Summarize: [Reasonable: 24.33%], [Bare: 16.94%], [Partial: 26.66%], [Heavy: 66.31%]\n",
      "9\n",
      "Summarize: [Reasonable: 58.58%], [Bare: 45.34%], [Partial: 57.62%], [Heavy: 81.27%]\n",
      "Summarize: [Reasonable: 23.87%], [Bare: 16.79%], [Partial: 25.82%], [Heavy: 65.86%]\n",
      "10\n",
      "Summarize: [Reasonable: 58.49%], [Bare: 45.30%], [Partial: 57.44%], [Heavy: 81.18%]\n",
      "Summarize: [Reasonable: 23.68%], [Bare: 16.67%], [Partial: 25.63%], [Heavy: 65.65%]\n",
      "11\n",
      "Summarize: [Reasonable: 58.70%], [Bare: 45.46%], [Partial: 57.63%], [Heavy: 81.19%]\n",
      "Summarize: [Reasonable: 23.78%], [Bare: 16.77%], [Partial: 25.58%], [Heavy: 65.64%]\n",
      "12\n",
      "Summarize: [Reasonable: 58.66%], [Bare: 45.39%], [Partial: 57.60%], [Heavy: 81.21%]\n",
      "Summarize: [Reasonable: 23.75%], [Bare: 16.74%], [Partial: 25.60%], [Heavy: 65.72%]\n",
      "13\n",
      "Summarize: [Reasonable: 58.62%], [Bare: 45.28%], [Partial: 57.50%], [Heavy: 81.15%]\n",
      "Summarize: [Reasonable: 23.68%], [Bare: 16.73%], [Partial: 25.53%], [Heavy: 65.66%]\n",
      "14\n",
      "Summarize: [Reasonable: 58.64%], [Bare: 45.29%], [Partial: 57.54%], [Heavy: 81.16%]\n",
      "Summarize: [Reasonable: 23.70%], [Bare: 16.69%], [Partial: 25.50%], [Heavy: 65.65%]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    print(epoch)\n",
    "    val(r\"C:\\Users\\mahdi\\Desktop\\Pedestrian-Detection-master\\val_crowdhuman.json\", r'_temp_val\\_temp_val_crowdhuman_%d.json'%epoch)\n",
    "    val(r\"data\\citypersons\\annotations\\val_gt.json\", r'_temp_val\\_temp_val_citypersons_%d.json'%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "500/500"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '_temp_eval/_temp_val_crowdhuman_EWC_citypersons.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mahdi\\Desktop\\Pedestrian-Detection-master\\Pedestrian_CL_CrowdHuman_to_CityPersons.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mahdi/Desktop/Pedestrian-Detection-master/Pedestrian_CL_CrowdHuman_to_CityPersons.ipynb#X22sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m%d\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(test_loader)), end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mahdi/Desktop/Pedestrian-Detection-master/Pedestrian_CL_CrowdHuman_to_CityPersons.ipynb#X22sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m             sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mahdi/Desktop/Pedestrian-Detection-master/Pedestrian_CL_CrowdHuman_to_CityPersons.ipynb#X22sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m_temp_eval/_temp_val_crowdhuman_EWC_citypersons.json\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mahdi/Desktop/Pedestrian-Detection-master/Pedestrian_CL_CrowdHuman_to_CityPersons.ipynb#X22sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(res, f)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mahdi/Desktop/Pedestrian-Detection-master/Pedestrian_CL_CrowdHuman_to_CityPersons.ipynb#X22sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m res \u001b[39m=\u001b[39m val(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mcitypersons\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mannotations\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mval_gt.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_temp_eval/_temp_val_crowdhuman_EWC_citypersons.json\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '_temp_eval/_temp_val_crowdhuman_EWC_citypersons.json'"
     ]
    }
   ],
   "source": [
    "#test data loader fo citypersons\n",
    "class ImageDataset_test(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, annotation, transforms):\n",
    "    self.root = root\n",
    "    self.coco = COCO(annotation)\n",
    "    self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # image ID\n",
    "    img_id = self.ids[idx]\n",
    "    # image file_name\n",
    "    img_file = self.coco.loadImgs(img_id)[0][\"im_name\"]\n",
    "    # read_imgae\n",
    "    img = Image.open(os.path.join(self.root, img_file))\n",
    "\n",
    "    # transforms\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms(img)\n",
    "    \n",
    "    return img\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.ids)\n",
    "\n",
    "test_data_path = r'data\\citypersons\\images\\val'\n",
    "coco_path_test = r\"data\\citypersons\\annotations\\val_gt.json\"\n",
    "test_dataset = ImageDataset_test(root=test_data_path, annotation=coco_path_test, transforms=get_transforms(train=False))\n",
    "\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, 1, shuffle=False)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "model = model.to(device)\n",
    "\n",
    "res = []\n",
    "i = 0\n",
    "for _ in range(1):\n",
    "    for i, imgs in enumerate(test_loader):\n",
    "        model.eval()\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        output = model(imgs)\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        num_people = len(scores[scores > 0.25])\n",
    "        boxes = output[0]['boxes'].detach().cpu().numpy()\n",
    "        boxes = boxes[:num_people]\n",
    "        if len(boxes) > 0:\n",
    "            boxes[:, [2, 3]] -= boxes[:, [0, 1]]\n",
    "\n",
    "            for ii,box in enumerate(boxes):\n",
    "                temp = {}\n",
    "                temp['image_id'] = i+1\n",
    "                temp['category_id'] = 1\n",
    "                temp['bbox'] = box[:4].tolist()\n",
    "                temp['score'] = np.float(scores[ii])\n",
    "                res.append(temp)\n",
    "                \n",
    "\n",
    "            print('\\r%d/%d' % (i + 1, len(test_loader)), end='')\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "\n",
    "with open('_temp_val/_temp_val_crowdhuman_EWC_citypersons.json', 'w') as f:\n",
    "    json.dump(res, f)\n",
    "\n",
    "\n",
    "res = val(r\"data\\citypersons\\annotations\\val_gt.json\", r'_temp_val/_temp_val_crowdhuman_EWC_citypersons.json')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.74s)\n",
      "creating index...\n",
      "index created!\n",
      "4370/4370Summarize: [Reasonable: 58.51%], [Bare: 45.56%], [Partial: 57.54%], [Heavy: 81.00%]\n",
      "0.5850768791588195\n"
     ]
    }
   ],
   "source": [
    "#test data loader\n",
    "class ImageDataset_test(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, annotation, transforms):\n",
    "    self.root = root\n",
    "    self.coco = COCO(annotation)\n",
    "    self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # image ID\n",
    "    img_id = self.ids[idx]\n",
    "    # image file_name\n",
    "    img_file = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "    # read_imgae\n",
    "    img = Image.open(os.path.join(self.root, img_file))\n",
    "\n",
    "    # transforms\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms(img)\n",
    "    \n",
    "    return img\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.ids)\n",
    "\n",
    "test_data_path = r'data\\CrowdHuman\\images\\val'\n",
    "coco_path_test = r\"data\\CrowdHuman\\val.json\"\n",
    "test_dataset = ImageDataset_test(root=test_data_path, annotation=coco_path_test, transforms=get_transforms(train=False))\n",
    "\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, 1, shuffle=False)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "model = model.to(device)\n",
    "\n",
    "res = []\n",
    "i = 0\n",
    "for _ in range(1):\n",
    "    for i, imgs in enumerate(test_loader):\n",
    "        model.eval()\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        output = model(imgs)\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        num_people = len(scores[scores > 0.25])\n",
    "        boxes = output[0]['boxes'].detach().cpu().numpy()\n",
    "        boxes = boxes[:num_people]\n",
    "        if len(boxes) > 0:\n",
    "            boxes[:, [2, 3]] -= boxes[:, [0, 1]]\n",
    "\n",
    "            for ii,box in enumerate(boxes):\n",
    "                temp = {}\n",
    "                temp['image_id'] = i+1\n",
    "                temp['category_id'] = 1\n",
    "                temp['bbox'] = box[:4].tolist()\n",
    "                temp['score'] = np.float(scores[ii])\n",
    "                res.append(temp)\n",
    "                \n",
    "\n",
    "            print('\\r%d/%d' % (i + 1, len(test_loader)), end='')\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "\n",
    "with open('_temp_val/_temp_val_crowdhuman_EWC_citypersons.json', 'w') as f:\n",
    "    json.dump(res, f)\n",
    "\n",
    "\n",
    "res = val(r\"C:\\Users\\mahdi\\Desktop\\Pedestrian-Detection-master\\val_crowdhuman.json\", r'_temp_val/_temp_val_crowdhuman_EWC_citypersons.json')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_city.eval_script.eval_demo import validate\n",
    "\n",
    "def val(gt_json_path, pred_json_path, log=None):\n",
    "    MRs = validate(gt_json_path, pred_json_path)\n",
    "    print('Summarize: [Reasonable: %.2f%%], [Bare: %.2f%%], [Partial: %.2f%%], [Heavy: %.2f%%]'\n",
    "          % (MRs[0]*100, MRs[1]*100, MRs[2]*100, MRs[3]*100))\n",
    "    if log is not None:\n",
    "        log.write(\"%.7f %.7f %.7f %.7f\\n\" % tuple(MRs))\n",
    "    return MRs[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1 (default, Mar  2 2020, 13:06:26) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3b09f0dae079356b11e2992c8ce1698bd60fda55aea4c87f004ec164747e9c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
