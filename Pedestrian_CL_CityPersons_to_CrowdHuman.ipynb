{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otrZdXl58mPH"
   },
   "source": [
    "# Continual Learning for pedestrian detection (CityPersons to CrowdHuman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rfI2u3bMYWOL",
    "outputId": "0fb5a812-babd-4d3c-bcc3-ecef75645553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cuda_enable = True\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda:0\")\n",
    "  print(\"GPU\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print(\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pRHpDpHtYWOQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nls0kzaOYWOY"
   },
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, annotation, transforms=None):\n",
    "    self.root = root\n",
    "    self.transforms = transforms\n",
    "    self.coco = COCO(annotation)\n",
    "    self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # image ID\n",
    "    img_id = self.ids[idx]\n",
    "    # image file_name\n",
    "    img_file = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "    # read_imgae\n",
    "    img = Image.open(os.path.join(self.root, img_file))\n",
    "    # get annotation ID\n",
    "    ann_ids = self.coco.getAnnIds(imgIds = img_id)\n",
    "    # read annotation\n",
    "    anns = self.coco.loadAnns(ann_ids)\n",
    "    # num of people in the picture\n",
    "    num_objs = len(anns)\n",
    "    # build information about bounding box & area\n",
    "    boxes = []\n",
    "    areas = []\n",
    "    for i in range(num_objs):\n",
    "      x_min = anns[i]['bbox'][0]\n",
    "      y_min = anns[i]['bbox'][1]\n",
    "      x_max = x_min + anns[i]['bbox'][2]\n",
    "      y_max = y_min + anns[i]['bbox'][3]\n",
    "      boxes.append([x_min, y_min, x_max, y_max])\n",
    "  \n",
    "    # transfer information to Tensor\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    labels = torch.ones((num_objs,), dtype = torch.int64)\n",
    "    img_id = torch.tensor([img_id])\n",
    "    areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "    # Annotation in dict form\n",
    "    Annotations = {\n",
    "        \"boxes\" : boxes,\n",
    "        \"labels\" : labels,\n",
    "        \"image_id\" : img_id,\n",
    "        \"iscrowd\" : iscrowd\n",
    "    }\n",
    "\n",
    "    # transforms\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms(img)\n",
    "    \n",
    "    return img, Annotations\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def get_transforms(train):\n",
    "  trans = []\n",
    "  if train:\n",
    "    trans.append(transforms.RandomHorizontalFlip(0.5))\n",
    "    # trans.append(transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)))\n",
    "    # trans.append(transforms.ColorJitter(brightness=1, contrast=1, saturation=1))\n",
    "  trans.append(transforms.ToTensor())\n",
    "  return transforms.Compose(trans)\n",
    "\n",
    "train_data_path = r'data\\citypersons\\images'\n",
    "coco_path = r\"data\\citypersons\\annotations\\train.json\"\n",
    "train_dataset = ImageDataset(root=train_data_path, annotation=coco_path, transforms=get_transforms(train=True))\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, 2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "8368d0de414a43fda4371db38049c8c9",
      "26a9ac1381cc444aa4945a5228cd8380",
      "632457719d764816a0de63b8510dc1ff",
      "059178c6cca742f9b1fa3085bf860c7e",
      "9075c2aa0716420faf648e8732df91f2",
      "cad656ce7d7e4bd995f386c4fdbcd632",
      "37f60a525f7f4246b9469f93c4610c32",
      "5dd3d0626c9b4171ba7a1e1a21ac273c"
     ]
    },
    "colab_type": "code",
    "id": "P-lnDZIEYWOg",
    "outputId": "8d715eac-6efa-4b63-9248-95c9c20418bb"
   },
   "outputs": [],
   "source": [
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "# change question to binary classification (human, not human)\n",
    "num_classes = 2\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the weights of the model\n",
    "model.load_state_dict(torch.load('ckpt/FRCNN_citypersons.pth'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#test data loader\n",
    "class ImageDataset_test(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, annotation, transforms):\n",
    "    self.root = root\n",
    "    self.coco = COCO(annotation)\n",
    "    self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # image ID\n",
    "    img_id = self.ids[idx]\n",
    "    # image file_name\n",
    "    img_file = self.coco.loadImgs(img_id)[0][\"im_name\"]\n",
    "    # read_imgae\n",
    "    img = Image.open(os.path.join(self.root, img_file))\n",
    "\n",
    "    # transforms\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms(img)\n",
    "    \n",
    "    return img\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.ids)\n",
    "\n",
    "test_data_path = r'data\\citypersons\\images\\val'\n",
    "coco_path_test = r\"data\\citypersons\\annotations\\val_gt.json\"\n",
    "test_dataset = ImageDataset_test(root=test_data_path, annotation=coco_path_test, transforms=get_transforms(train=False))\n",
    "\n",
    "    \n",
    "test_loader_city = torch.utils.data.DataLoader(test_dataset, 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.61s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#test data loader\n",
    "class ImageDataset_test(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, annotation, transforms):\n",
    "    self.root = root\n",
    "    self.coco = COCO(annotation)\n",
    "    self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # image ID\n",
    "    img_id = self.ids[idx]\n",
    "    # image file_name\n",
    "    img_file = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "    # read_imgae\n",
    "    img = Image.open(os.path.join(self.root, img_file))\n",
    "\n",
    "    # transforms\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms(img)\n",
    "    \n",
    "    return img\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.ids)\n",
    "\n",
    "test_data_path = r'data\\CrowdHuman\\images\\val'\n",
    "coco_path_test = r\"data\\CrowdHuman\\val.json\"\n",
    "test_dataset = ImageDataset_test(root=test_data_path, annotation=coco_path_test, transforms=get_transforms(train=False))\n",
    "\n",
    "    \n",
    "test_loader_crowd = torch.utils.data.DataLoader(test_dataset, 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def log_output(model,test_loader,epoch,dataset = \"crowdhuman\"):\n",
    "\n",
    "        \n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    res = []\n",
    "    i = 0\n",
    "    for _ in range(1):\n",
    "        for i, imgs in enumerate(test_loader):\n",
    "            model.eval()\n",
    "            imgs = list(img.to(device) for img in imgs)\n",
    "            output = model(imgs)\n",
    "            scores = output[0]['scores'].detach().cpu().numpy()\n",
    "            num_people = len(scores[scores > 0.25])\n",
    "            boxes = output[0]['boxes'].detach().cpu().numpy()\n",
    "            boxes = boxes[:num_people]\n",
    "            if len(boxes) > 0:\n",
    "                boxes[:, [2, 3]] -= boxes[:, [0, 1]]\n",
    "\n",
    "                for ii,box in enumerate(boxes):\n",
    "                    temp = {}\n",
    "                    temp['image_id'] = i+1\n",
    "                    temp['category_id'] = 1\n",
    "                    temp['bbox'] = box[:4].tolist()\n",
    "                    temp['score'] = np.float(scores[ii])\n",
    "                    res.append(temp)\n",
    "                    \n",
    "\n",
    "                print('\\r%d/%d' % (i + 1, len(test_loader)), end='')\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "\n",
    "    with open('_temp_val\\_temp_val_' + dataset + '_%d.json'%epoch, 'w') as f:\n",
    "        json.dump(res, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import autograd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from barbar import Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import isnan\n",
    "\n",
    "\n",
    "class ElasticWeightConsolidation:\n",
    "\n",
    "    def __init__(self, model, weight=1000000):\n",
    "        self.model = model\n",
    "        self.model.requires_grad_(True)\n",
    "        self.weight = weight\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        self.optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "\n",
    "    def _update_mean_params(self):\n",
    "        for param_name, param in self.model.named_parameters():\n",
    "            _buff_param_name = param_name.replace('.', '__')\n",
    "            self.model.register_buffer(_buff_param_name+'_estimated_mean', param.data.clone())\n",
    "\n",
    "    def _update_fisher_params(self, current_ds, grad_avg_path = None):\n",
    "        dl = current_ds\n",
    "        model.requires_grad_(True)\n",
    "        model.train()\n",
    "        if grad_avg_path is not None:\n",
    "            self.grad_avg = np.load(grad_avg_path, allow_pickle=True)\n",
    "        else:\n",
    "            i = 0\n",
    "            for imgs, annotations in Bar(dl):\n",
    "                imgs = list(img.to(device) for img in imgs)\n",
    "                annotations = [{k:v.to(device) for k, v in t.items()} for t in annotations]\n",
    "\n",
    "                train_flag = False\n",
    "                if len(annotations[0]['boxes'].shape) == 2 or annotations[0]['boxes'].shape[-1] == 4:\n",
    "                    if len(annotations) > 1:\n",
    "                        if (len(annotations[1]['boxes'].shape) == 2 or annotations[1]['boxes'].shape[-1] == 4):\n",
    "                            train_flag = True\n",
    "                    else:\n",
    "                        train_flag = True\n",
    "\n",
    "                if train_flag:\n",
    "                    loss_dict = self.model(imgs, annotations)\n",
    "                    losses = sum(loss for loss in loss_dict.values())  \n",
    "                    log_likelihood = torch.log(losses)\n",
    "                    grad_log_liklihood = autograd.grad(log_likelihood, self.model.parameters())\n",
    "                    grad_log_liklihood = [g.detach().cpu().numpy() for g in grad_log_liklihood]\n",
    "                    # grad_list.append(grad_log_liklihood)\n",
    "\n",
    "                    if i == 0:\n",
    "                        grad_sum = [np.zeros(g.shape) for g in grad_log_liklihood]\n",
    "                    \n",
    "                    for ii in range(len(grad_log_liklihood)):\n",
    "                        grad_sum[ii] += grad_log_liklihood[ii]\n",
    "\n",
    "                    i += 1\n",
    "\n",
    "\n",
    "            self.grad_avg = [g/i for g in grad_sum]\n",
    "        \n",
    "        _buff_param_names = [param[0].replace('.', '__') for param in self.model.named_parameters()]\n",
    "        for _buff_param_name, param in zip(_buff_param_names, self.grad_avg):\n",
    "            self.model.register_buffer(_buff_param_name+'_estimated_fisher', torch.tensor(param).data.clone() ** 2)\n",
    "\n",
    "    def register_ewc_params(self, dataset, grad_avg_path = None):\n",
    "        self._update_fisher_params(dataset, grad_avg_path)\n",
    "        self._update_mean_params()\n",
    "\n",
    "    def _compute_consolidation_loss(self, weight):\n",
    "        try:\n",
    "            losses = []\n",
    "            for param_name, param in self.model.named_parameters():\n",
    "                _buff_param_name = param_name.replace('.', '__')\n",
    "                estimated_mean = getattr(self.model, '{}_estimated_mean'.format(_buff_param_name)).cuda()\n",
    "                estimated_fisher = getattr(self.model, '{}_estimated_fisher'.format(_buff_param_name)).cuda()\n",
    "                \n",
    "                losses.append((estimated_fisher * (param - estimated_mean) ** 2).sum())\n",
    "            return (weight/2) * sum(losses)\n",
    "        except AttributeError:\n",
    "            return 0\n",
    "\n",
    "    def forward_backward_update(self, input, target):\n",
    "        output = self.model(input)\n",
    "        loss = self._compute_consolidation_loss(self.weight) + self.crit(output, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def Fine_tune_new_data(self, Next_ds, epoch = 15):\n",
    "        params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                    momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                        step_size=3,\n",
    "                                                        gamma=0.1)\n",
    "\n",
    "        epochs = 15\n",
    "        best_epoch = 0\n",
    "        self.model = self.model.to(device)\n",
    "        min_loss = 10000\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            i = 0\n",
    "            print(\"Epoch: %d/%d\" % (epoch, epochs))\n",
    "            loss_sum = 0\n",
    "            for imgs, annotations in Bar(train_loader):\n",
    "                i += 1\n",
    "                imgs = list(img.to(device) for img in imgs)\n",
    "                annotations = [{k:v.to(device) for k, v in t.items()} for t in annotations]\n",
    "                \n",
    "                train_flag = False\n",
    "                if len(annotations[0]['boxes'].shape) == 2 or annotations[0]['boxes'].shape[-1] == 4:\n",
    "                    if len(annotations) > 1:\n",
    "                        if (len(annotations[1]['boxes'].shape) == 2 or annotations[1]['boxes'].shape[-1] == 4):\n",
    "                            train_flag = True\n",
    "                    else:\n",
    "                        train_flag = True\n",
    "\n",
    "                if train_flag:\n",
    "                    loss_dict = model(imgs, annotations)\n",
    "                    losses = 0.6*self._compute_consolidation_loss(self.weight) + 0.4*sum(loss for loss in loss_dict.values())\n",
    "                    optimizer.zero_grad()\n",
    "                    losses.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "                    optimizer.step()\n",
    "                    loss_sum += losses.item()\n",
    "                \n",
    "            print(\"Iteration: {}; Loss: {}\".format(i, loss_sum/i))\n",
    "            if loss_sum/i < min_loss:\n",
    "                min_loss = loss_sum/i\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), 'ckpt/FRCNN_crowdhuman_EWC_CityPersons.pth')\n",
    "                print(\"Model saved\")\n",
    "            print(\"Best epoch: {}; Best loss: {}\".format(best_epoch, min_loss))\n",
    "            lr_scheduler.step()\n",
    "            log_output(model,test_loader_crowd,epoch,dataset = \"crowdhuman\")\n",
    "            log_output(model,test_loader_city,epoch,dataset = \"citypersons\")\n",
    "\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.model, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.model = torch.load(filename)\n",
    "\n",
    "ewc = ElasticWeightConsolidation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewc.register_ewc_params(train_loader,'Average_gradients/gard_avg_citypersons.npy')\n",
    "# ewc.weight = 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('gard_avg_citypersons.npy', ewc.grad_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune with other dataset with EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=2.42s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, annotation, transforms=None):\n",
    "    self.root = root\n",
    "    self.transforms = transforms\n",
    "    self.coco = COCO(annotation)\n",
    "    self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # image ID\n",
    "    img_id = self.ids[idx]\n",
    "    # image file_name\n",
    "    img_file = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "    # read_imgae\n",
    "    img = Image.open(os.path.join(self.root, img_file))\n",
    "    # get annotation ID\n",
    "    ann_ids = self.coco.getAnnIds(imgIds = img_id)\n",
    "    # read annotation\n",
    "    anns = self.coco.loadAnns(ann_ids)\n",
    "    # num of people in the picture\n",
    "    num_objs = len(anns)\n",
    "    # build information about bounding box & area\n",
    "    boxes = []\n",
    "    areas = []\n",
    "    for i in range(num_objs):\n",
    "      x_min = anns[i]['bbox'][0]\n",
    "      y_min = anns[i]['bbox'][1]\n",
    "      x_max = x_min + anns[i]['bbox'][2]\n",
    "      y_max = y_min + anns[i]['bbox'][3]\n",
    "      boxes.append([x_min, y_min, x_max, y_max])\n",
    "  \n",
    "    # transfer information to Tensor\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    labels = torch.ones((num_objs,), dtype = torch.int64)\n",
    "    img_id = torch.tensor([img_id])\n",
    "    areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "    # Annotation in dict form\n",
    "    Annotations = {\n",
    "        \"boxes\" : boxes,\n",
    "        \"labels\" : labels,\n",
    "        \"image_id\" : img_id,\n",
    "        \"iscrowd\" : iscrowd\n",
    "    }\n",
    "\n",
    "    # transforms\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms(img)\n",
    "    \n",
    "    return img, Annotations\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.ids)\n",
    "\n",
    "def get_transforms(train):\n",
    "  trans = []\n",
    "  if train:\n",
    "    trans.append(transforms.RandomHorizontalFlip(0.5))\n",
    "    # trans.append(transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)))\n",
    "    # trans.append(transforms.ColorJitter(brightness=1, contrast=1, saturation=1))\n",
    "  trans.append(transforms.ToTensor())\n",
    "  return transforms.Compose(trans)\n",
    "\n",
    "train_data_path = r'data\\CrowdHuman\\images'\n",
    "coco_path = r\"C:\\Users\\mahdi\\Desktop\\Pedestrian-Detection-master\\data\\CrowdHuman\\train.json\"\n",
    "train_dataset = ImageDataset(root=train_data_path, annotation=coco_path, transforms=get_transforms(train=True))\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, 3, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15\n",
      "15000/15000: [===============================>] - ETA 0.4ssss\n",
      "Iteration: 5000; Loss: 0.4850561932629271\n",
      "Model saved\n",
      "Best epoch: 0; Best loss: 0.4850561932629271\n",
      "500/50070Epoch: 1/15\n",
      "15000/15000: [===============================>] - ETA 0.4ssss\n",
      "Iteration: 5000; Loss: 0.4851483638084053\n",
      "Best epoch: 0; Best loss: 0.4850561932629271\n",
      "500/50070Epoch: 2/15\n",
      "15000/15000: [===============================>] - ETA 0.7ssss\n",
      "Iteration: 5000; Loss: 0.4910410889509254\n",
      "Best epoch: 0; Best loss: 0.4850561932629271\n",
      "500/50070Epoch: 3/15\n",
      "15000/15000: [===============================>] - ETA 0.4ssss\n",
      "Iteration: 5000; Loss: 0.4812864754705712\n",
      "Model saved\n",
      "Best epoch: 3; Best loss: 0.4812864754705712\n",
      "500/50070Epoch: 4/15\n",
      "15000/15000: [===============================>] - ETA 0.4ssss\n",
      "Iteration: 5000; Loss: 0.4795068822265791\n",
      "Model saved\n",
      "Best epoch: 4; Best loss: 0.4795068822265791\n",
      "500/50070Epoch: 5/15\n",
      "15000/15000: [===============================>] - ETA 0.5ssss\n",
      "Iteration: 5000; Loss: 0.47847487890427426\n",
      "Model saved\n",
      "Best epoch: 5; Best loss: 0.47847487890427426\n",
      "500/50070Epoch: 6/15\n",
      "15000/15000: [===============================>] - ETA 0.4ssss\n",
      "Iteration: 5000; Loss: 0.4769222983089317\n",
      "Model saved\n",
      "Best epoch: 6; Best loss: 0.4769222983089317\n",
      "500/50070Epoch: 7/15\n",
      "15000/15000: [===============================>] - ETA 0.4ssss\n",
      "Iteration: 5000; Loss: 0.47716421344670906\n",
      "Best epoch: 6; Best loss: 0.4769222983089317\n",
      "499/50070Epoch: 8/15\n",
      "15000/15000: [===============================>] - ETA 0.4ssss\n",
      "Iteration: 5000; Loss: 0.4769671285126666\n",
      "Best epoch: 6; Best loss: 0.4769222983089317\n",
      "500/50070Epoch: 9/15\n",
      "15000/15000: [===============================>] - ETA 0.4ssss\n",
      "Iteration: 5000; Loss: 0.4781696449328182\n",
      "Best epoch: 6; Best loss: 0.4769222983089317\n",
      "500/50070Epoch: 10/15\n",
      "15000/15000: [===============================>] - ETA 0.4ssss\n",
      "Iteration: 5000; Loss: 0.477014438974258\n",
      "Best epoch: 6; Best loss: 0.4769222983089317\n",
      "500/50070Epoch: 11/15\n",
      "15000/15000: [===============================>] - ETA 0.6ssss\n",
      "Iteration: 5000; Loss: 0.4765212124898963\n",
      "Model saved\n",
      "Best epoch: 11; Best loss: 0.4765212124898963\n",
      "500/50070Epoch: 12/15\n",
      "15000/15000: [===============================>] - ETA 1.0ssss\n",
      "Iteration: 5000; Loss: 0.4794391366019395\n",
      "Best epoch: 11; Best loss: 0.4765212124898963\n",
      "500/50070Epoch: 13/15\n",
      "15000/15000: [===============================>] - ETA 0.4ssss\n",
      "Iteration: 5000; Loss: 0.4768612977568224\n",
      "Best epoch: 11; Best loss: 0.4765212124898963\n",
      "500/50070Epoch: 14/15\n",
      "15000/15000: [===============================>] - ETA 0.3ssss\n",
      "Iteration: 5000; Loss: 0.4771147440387703\n",
      "Best epoch: 11; Best loss: 0.4765212124898963\n",
      "500/50070"
     ]
    }
   ],
   "source": [
    "ewc.Fine_tune_new_data(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the weights of the model\n",
    "model.load_state_dict(torch.load('ckpt\\FRCNN_crowdhuman_EWC_CityPersons.pth'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_demo import validate\n",
    "\n",
    "def val(gt_json_path, pred_json_path, log=None):\n",
    "    MRs = validate(gt_json_path, pred_json_path)\n",
    "    print('Summarize: [Reasonable: %.2f%%], [Bare: %.2f%%], [Partial: %.2f%%], [Heavy: %.2f%%]'\n",
    "          % (MRs[0]*100, MRs[1]*100, MRs[2]*100, MRs[3]*100))\n",
    "    if log is not None:\n",
    "        log.write(\"%.7f %.7f %.7f %.7f\\n\" % tuple(MRs))\n",
    "    return MRs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Summarize: [Reasonable: 58.45%], [Bare: 42.12%], [Partial: 53.25%], [Heavy: 80.53%]\n",
      "Summarize: [Reasonable: 35.91%], [Bare: 27.04%], [Partial: 40.32%], [Heavy: 77.31%]\n",
      "1\n",
      "Summarize: [Reasonable: 56.31%], [Bare: 42.66%], [Partial: 51.99%], [Heavy: 75.39%]\n",
      "Summarize: [Reasonable: 31.42%], [Bare: 23.51%], [Partial: 34.85%], [Heavy: 74.25%]\n",
      "2\n",
      "Summarize: [Reasonable: 54.09%], [Bare: 40.20%], [Partial: 53.01%], [Heavy: 78.12%]\n",
      "Summarize: [Reasonable: 35.54%], [Bare: 27.06%], [Partial: 39.76%], [Heavy: 78.36%]\n",
      "3\n",
      "Summarize: [Reasonable: 54.93%], [Bare: 40.66%], [Partial: 49.64%], [Heavy: 75.37%]\n",
      "Summarize: [Reasonable: 32.72%], [Bare: 24.25%], [Partial: 36.29%], [Heavy: 75.45%]\n",
      "4\n",
      "Summarize: [Reasonable: 57.19%], [Bare: 42.61%], [Partial: 50.30%], [Heavy: 75.02%]\n",
      "Summarize: [Reasonable: 30.59%], [Bare: 22.35%], [Partial: 33.82%], [Heavy: 74.43%]\n",
      "5\n",
      "Summarize: [Reasonable: 56.70%], [Bare: 42.03%], [Partial: 50.51%], [Heavy: 75.01%]\n",
      "Summarize: [Reasonable: 33.02%], [Bare: 24.84%], [Partial: 36.12%], [Heavy: 75.41%]\n",
      "6\n",
      "Summarize: [Reasonable: 56.23%], [Bare: 41.84%], [Partial: 49.77%], [Heavy: 74.68%]\n",
      "Summarize: [Reasonable: 32.52%], [Bare: 24.48%], [Partial: 35.77%], [Heavy: 74.42%]\n",
      "7\n",
      "Summarize: [Reasonable: 56.45%], [Bare: 42.22%], [Partial: 50.05%], [Heavy: 74.89%]\n",
      "Summarize: [Reasonable: 32.54%], [Bare: 24.33%], [Partial: 35.93%], [Heavy: 74.56%]\n",
      "8\n",
      "Summarize: [Reasonable: 56.12%], [Bare: 41.69%], [Partial: 49.72%], [Heavy: 74.56%]\n",
      "Summarize: [Reasonable: 32.62%], [Bare: 24.23%], [Partial: 36.27%], [Heavy: 74.67%]\n",
      "9\n",
      "Summarize: [Reasonable: 56.02%], [Bare: 41.73%], [Partial: 49.97%], [Heavy: 74.60%]\n",
      "Summarize: [Reasonable: 32.64%], [Bare: 24.30%], [Partial: 36.09%], [Heavy: 74.80%]\n",
      "10\n",
      "Summarize: [Reasonable: 55.96%], [Bare: 41.64%], [Partial: 49.71%], [Heavy: 74.66%]\n",
      "Summarize: [Reasonable: 32.62%], [Bare: 24.26%], [Partial: 36.05%], [Heavy: 74.79%]\n",
      "11\n",
      "Summarize: [Reasonable: 64.22%], [Bare: 47.79%], [Partial: 67.63%], [Heavy: 91.58%]\n",
      "Summarize: [Reasonable: 22.74%], [Bare: 15.00%], [Partial: 25.55%], [Heavy: 67.56%]\n",
      "12\n",
      "Summarize: [Reasonable: 64.24%], [Bare: 47.79%], [Partial: 67.64%], [Heavy: 91.58%]\n",
      "Summarize: [Reasonable: 22.75%], [Bare: 15.01%], [Partial: 25.54%], [Heavy: 67.56%]\n",
      "13\n",
      "Summarize: [Reasonable: 64.23%], [Bare: 47.78%], [Partial: 67.64%], [Heavy: 91.58%]\n",
      "Summarize: [Reasonable: 22.78%], [Bare: 14.99%], [Partial: 25.54%], [Heavy: 67.54%]\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    print(epoch)\n",
    "    val(r\"C:\\Users\\mahdi\\Desktop\\Pedestrian-Detection-master\\val_crowdhuman.json\", r'_temp_val\\_temp_val_crowdhuman_%d.json'%epoch)\n",
    "    val(r\"data\\citypersons\\annotations\\val_gt.json\", r'_temp_val\\_temp_val_citypersons_%d.json'%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "500/500Summarize: [Reasonable: 32.65%], [Bare: 23.56%], [Partial: 36.39%], [Heavy: 74.39%]\n",
      "0.3265194158684835\n"
     ]
    }
   ],
   "source": [
    "#test data loader fo citypersons\n",
    "class ImageDataset_test(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, annotation, transforms):\n",
    "    self.root = root\n",
    "    self.coco = COCO(annotation)\n",
    "    self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # image ID\n",
    "    img_id = self.ids[idx]\n",
    "    # image file_name\n",
    "    img_file = self.coco.loadImgs(img_id)[0][\"im_name\"]\n",
    "    # read_imgae\n",
    "    img = Image.open(os.path.join(self.root, img_file))\n",
    "\n",
    "    # transforms\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms(img)\n",
    "    \n",
    "    return img\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.ids)\n",
    "\n",
    "test_data_path = r'data\\citypersons\\images\\val'\n",
    "coco_path_test = r\"data\\citypersons\\annotations\\val_gt.json\"\n",
    "test_dataset = ImageDataset_test(root=test_data_path, annotation=coco_path_test, transforms=get_transforms(train=False))\n",
    "\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, 1, shuffle=False)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "model = model.to(device)\n",
    "\n",
    "res = []\n",
    "i = 0\n",
    "for _ in range(1):\n",
    "    for i, imgs in enumerate(test_loader):\n",
    "        model.eval()\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        output = model(imgs)\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        num_people = len(scores[scores > 0.25])\n",
    "        boxes = output[0]['boxes'].detach().cpu().numpy()\n",
    "        boxes = boxes[:num_people]\n",
    "        if len(boxes) > 0:\n",
    "            boxes[:, [2, 3]] -= boxes[:, [0, 1]]\n",
    "\n",
    "            for ii,box in enumerate(boxes):\n",
    "                temp = {}\n",
    "                temp['image_id'] = i+1\n",
    "                temp['category_id'] = 1\n",
    "                temp['bbox'] = box[:4].tolist()\n",
    "                temp['score'] = np.float(scores[ii])\n",
    "                res.append(temp)\n",
    "                \n",
    "\n",
    "            print('\\r%d/%d' % (i + 1, len(test_loader)), end='')\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "\n",
    "with open('_temp_val/_temp_val_crowdhuman_EWC_citypersons.json', 'w') as f:\n",
    "    json.dump(res, f)\n",
    "\n",
    "\n",
    "res = val(r\"data\\citypersons\\annotations\\val_gt.json\", r'_temp_val/_temp_val_crowdhuman_EWC_citypersons.json')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.41s)\n",
      "creating index...\n",
      "index created!\n",
      "4370/4370Summarize: [Reasonable: 58.96%], [Bare: 43.91%], [Partial: 56.80%], [Heavy: 82.20%]\n",
      "0.589617762516651\n"
     ]
    }
   ],
   "source": [
    "#test data loader\n",
    "class ImageDataset_test(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, annotation, transforms):\n",
    "    self.root = root\n",
    "    self.coco = COCO(annotation)\n",
    "    self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # image ID\n",
    "    img_id = self.ids[idx]\n",
    "    # image file_name\n",
    "    img_file = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "    # read_imgae\n",
    "    img = Image.open(os.path.join(self.root, img_file))\n",
    "\n",
    "    # transforms\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms(img)\n",
    "    \n",
    "    return img\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.ids)\n",
    "\n",
    "test_data_path = r'data\\CrowdHuman\\images\\val'\n",
    "coco_path_test = r\"data\\CrowdHuman\\val.json\"\n",
    "test_dataset = ImageDataset_test(root=test_data_path, annotation=coco_path_test, transforms=get_transforms(train=False))\n",
    "\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, 1, shuffle=False)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "model = model.to(device)\n",
    "\n",
    "res = []\n",
    "i = 0\n",
    "for _ in range(1):\n",
    "    for i, imgs in enumerate(test_loader):\n",
    "        model.eval()\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        output = model(imgs)\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        num_people = len(scores[scores > 0.25])\n",
    "        boxes = output[0]['boxes'].detach().cpu().numpy()\n",
    "        boxes = boxes[:num_people]\n",
    "        if len(boxes) > 0:\n",
    "            boxes[:, [2, 3]] -= boxes[:, [0, 1]]\n",
    "\n",
    "            for ii,box in enumerate(boxes):\n",
    "                temp = {}\n",
    "                temp['image_id'] = i+1\n",
    "                temp['category_id'] = 1\n",
    "                temp['bbox'] = box[:4].tolist()\n",
    "                temp['score'] = np.float(scores[ii])\n",
    "                res.append(temp)\n",
    "                \n",
    "\n",
    "            print('\\r%d/%d' % (i + 1, len(test_loader)), end='')\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "\n",
    "with open('_temp_val/_temp_val_citypersons_EWC_crowdhuman.json', 'w') as f:\n",
    "    json.dump(res, f)\n",
    "\n",
    "\n",
    "res = val(r\"C:\\Users\\mahdi\\Desktop\\Pedestrian-Detection-master\\val_crowdhuman.json\", r'_temp_val\\_temp_val_citypersons_EWC_crowdhuman.json')\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1 (default, Mar  2 2020, 13:06:26) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3b09f0dae079356b11e2992c8ce1698bd60fda55aea4c87f004ec164747e9c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
